\section{Estadística}

%%%%%%%%%%%%1872%%%%%%%%%%%
\subsection{\sffamily Análisis y Ajuste de Mixturas Gaussianas {\footnotesize (FALTA, FALTA)}} \label{reg-1872} \index{Cuevas Covarrubias Carlos}
\noindent {\bfseries Carlos  Cuevas Covarrubias}, {\tt ccuevas@anahuac.mx}  {\slshape (Universidad Anáhuac)}\\
\noindent {\it  Coautor: Jorge  Rosales Contreras        }\\
\noindent Esta presentación estará dirigida a estudiantes y académicos interesados, tanto en la estadística matemática como en sus aplicaciones. Ofrecerá una introducción sencilla al estudio de las mezclas de funciones de distribución con especial énfasis en el caso continuo. Comenzaremos con un breve recuento del origen histórico que motivó el uso de los modelos de mixturas. Luego, analizaremos algunos conceptos fundamentales de la Estadística Matemática y paulatinamente centramos nuestra atención en un problema específico: el ajuste de funciones de distribución continuas por medio de mixturas gaussianas. A lo largo de la presentación describiremos el algoritmo {\bf EM} para maximización de funciones de verosimilitud y mostraremos su implementación computacional con ejemplos sencillos. Discutiremos también sobre algunos criterios de análisis exploratorio y de bondad de ajuste que permiten evaluar el potencial de diversos modelos de mixturas en contextos especí?cos. Finalmente, las ideas presentadas serán ilustradas con el análisis de ejemplos prácticos sobre datos reales del mercado ?nanciero mexicano.
%%%%%%%%%%%%226%%%%%%%%%%%
\subsection{\sffamily Evaluación de la Exactitud y Precisión de un Modelo con Regresión Lineal {\footnotesize (RT, 2Lic)}} \label{reg-226} \index{Balam Lizama Rosalinda Georgina}
\noindent {\bfseries Rosalinda Georgina Balam Lizama}, {\tt rosi\_bl85@yahoo.com.mx}  {\slshape (Universidad Autónoma de Yucatán, Facultad de Matemáticas)}\\
\noindent {\it  Coautores: Salvador  Medina Peralta, Luis  Colorado Martínez      }\\
\noindent La validación de un modelo es la comparación por medio de algún método de las predicciones del modelo con observaciones del sistema real para determinar su capacidad predictiva (McKinion y Baker, 1982). En la validación de un modelo se evalúa su exactitud y precisión. Una de las técnicas más comunes en la validación de modelos es la de Regresión Lineal Simple de los observados sobre los predichos (Mayer et al., 1994; Analla, 1998; Tedeschi, 2006). Esta técnica se encuentra principalmente sujeta al cumplimiento de sus supuestos. Cuando los residuales son independientes, se ajustan a una distribución normal y tienen varianza común; se aplican pruebas de hipótesis estadísticas para evaluar la exactitud, intercepto cero y pendiente uno, ya sea mediante pruebas t de Student, o bien, mediante una prueba F para determinar si el intercepto y la pendiente son simultáneamente cero y uno respectivamente. Adicional a dichas pruebas estadísticas, suelen presentarse: (i) el gráfico de dispersión de los valores predichos contra los observados, junto con la recta de regresión estimada y la recta determinística $y=z$, y (ii) el coeficiente de determinación ($R^2$) como indicador de precisión. Sin embargo, no siempre se cumplen los supuestos de normalidad y/o igualdad de varianzas, necesarios para realizar dichas pruebas estadísticas; además de que la precisión se evalúa de manera determinista, ya que no se proporciona un error para la estimación del coeficiente de determinación. Por lo tanto en este trabajo se planteará la metodología para evaluar la exactitud y precisión de un modelo basado en la técnica de regresión lineal con un enfoque de intervalos de confianza, para validar un modelo cuando se cumplan o no los supuestos tradicionales.
%%%%%%%%%%%%501%%%%%%%%%%%
\subsection{\sffamily Cálculo del p-valor en pruebas de bondad de ajuste {\footnotesize (RT, Pos)}} \label{reg-501} \index{Beltrán Beltrán Jesús Iván}
\noindent {\bfseries Jesús Iván Beltrán Beltrán}, {\tt eluncle@hotmail.com}  {\slshape (Universidad Autónoma Metropolitana. Iztapalapa)}\\
          \noindent En esta plática se exponen algunos métodos para las pruebas de bondad de ajuste discretas donde se hace uso del llamado proceso empírico. Se comenta como se realizan simulaciones de muestras condicionales con el uso de la función distribución de Rao-Blackwell, como en el caso en la Gaussiana inversa en O'Reilly y Gracia-Medrano (2006) o como lo es en los casos discretos discutidos por González Barrios et al. (Métrica, 2006, Vol 64), donde se hace uso de las herramientas computacionales existentes hoy en día. Se desarrolla la distribución Rao-Blackwell para la distribución de series de potencia, así como para sus casos particulares, los cuales son, la distribución binomial, binomial negativa y Poisson. Se propone una extensión de estadística de prueba función de la generadora de probabilidades. También se desarrolla la función distribución Rao-Blackwell para la binomial negativa generalizada.
%%%%%%%%%%%%1173%%%%%%%%%%%
\subsection{\sffamily Análisis de Componentes Principales para reducción de dimensión de datos de microarreglos con tiempos de supervivencia censurados {\footnotesize (CI, Pos)}} \label{reg-1173} \index{Bolívar Cimé Addy Margarita}
\noindent {\bfseries Addy Margarita Bolívar Cimé}, {\tt addy.bolivar@gmail.com}  {\slshape (Universidad de Rice, Departamento de Estadística)}\\
\noindent {\it  Coautor: Javier  Rojo        }\\
\noindent Los estudios de microarreglos ADN permiten llevar a cabo de forma rápida y eficiente análisis simultáneos de miles de genes en un sólo experimento, con el fin de conocer el comportamiento de estos bajo determinadas situaciones. Los datos de microarreglos son datos matriciales $p\times n$ donde $p$ representa el número de genes analizados y $n$ es el número de individuos estudiados. Debido a que se analiza una gran cantidad de genes (miles de ellos), en esta clase de datos $p$ es usualmente mucho más grande que $n$. Estos datos a menudo incluyen información de la supervivencia de los pacientes, por lo que es importante analizar los tiempos de supervivencia de los pacientes en términos de sus correspondientes niveles de expresión de genes. Una manera de hacer frente a la gran dimensión de los datos de migroarreglos es primero reducir la dimensión y posteriormente usar el ``modelo proporcional de Cox'' para estimar la función de supervivencia de los pacientes. En estudios recientes se han hecho comparaciones entre varios métodos de reducción de dimensión con el fin de saber cuáles de ellos tienen mejores propiedades en la estimación de la función de supervivencia, en presencia de tiempos de supervivencia censurados. Se ha visto que el Análisis de Componentes Principales (ACP), que es uno de los métodos más populares para reducir dimensión, tiene propiedades muy pobres en comparación con otros métodos de reducción de dimensión como Partial Least Squares (PLS) y Rank-based Modified Partial Least Squares (RMPLS). En esta plática se verá cuales son algunos de los factores que influyen en este mal comportamiento de ACP y también se darán algunas condiciones, en términos de los eigenvectores de la matriz de covarianza poblacional y el vector de coeficientes del modelo proporcional de Cox, bajo las cuales se espera que ACP se comporte mejor si se tiene una buena estimación de la matriz de covarianza de los datos.
%%%%%%%%%%%%903%%%%%%%%%%%
\subsection{\sffamily Probabilidad y estadística para simulación del sistema de juicios orales en el Estado de Guanajuato {\footnotesize (CDV, 2Lic)}} \label{reg-903} \index{Cecilio Ayala Erick Alberto}
\noindent {\bfseries Erick Alberto Cecilio Ayala}, {\tt erick@cimat.mx}  {\slshape (Centro de Investigación en Matemáticas (CIMAT))}\\
          \noindent El Poder Judicial del Estado de Guanajuato implementó el juicio de oralidad en materia penal a partir del 1 de septiembre de 2011, para cumplir con la reforma constitucional de 2008.Como instrumento auxiliar en la planeación de recursos físicos y humanos, fue desarrollado un modelo de simulación de Monte Carlo para valorar distintos escenarios de operación, como función del número de salas de oralidad y jueces de control. Como partes constitutivas, el modelo contiene modelos probabilísticos para describir comportamientos aleatorios en las consignaciones de delitos así como en los flujos procesales. Para especificarlos, se utilizaron estimadores de Máxima Verosimilitud para obtener índices de incidencia de delitos en la Región 1 del estado de Guanajuato. Estos índices se estimaron considerando tipos de delitos y fecha (hora, día, mes) de ocurrencia.
%%%%%%%%%%%%299%%%%%%%%%%%
\subsection{\sffamily Análisis de múltiples puntos de cambio {\footnotesize (CI, 2Lic)}} \label{reg-299} \index{Cordero Franco Álvaro Eduardo}
\noindent {\bfseries Álvaro Eduardo Cordero Franco}, {\tt lalo.cordero@gmail.com}  {\slshape (Universidad Autónoma de Nuevo León (UANL) Facultad de Ciencias Físico Matemáticas (FCFM) Centro de Investigación en Ciencias  Físico Matemáticas (CICFIM))}\\
          \noindent En esta investigación se analiza una serie de tiempo que sigue una distribución normal en la que se sospecha que ocurren múltiples cambios tanto en su media como en su varianza. El objetivo es estimar en que puntos ocurrieron dichos cambios; así como los parámetros de la distribución en cada momento. Para esto se aplica el método de máxima verosimilitud obteniendo una función entera a maximizar. Debido al gran número de iteraciones que se requieren para la búsqueda exhaustiva de la solución, se aplicó un algoritmo heurístico de construcción para aproximar la solución.
%%%%%%%%%%%%1608%%%%%%%%%%%
\subsection{\sffamily Comparación del método de Suavidad Controlada para elegir el parámetro de suavizamiento al estimar tendencias  con el filtro de Hodrick y Prescott. {\footnotesize (RT, Pos)}} \label{reg-1608} \index{Guerrero Guzmán Víctor Manuel}
\noindent {\bfseries Víctor Manuel Guerrero Guzmán}, {\tt guerrero@itam.mx}  {\slshape (Departamento de Estadística, Instituto Tecnológico Autónomo de México (ITAM).)}\\
\noindent {\it  Coautores: Daniela  Cortés Toto, Hortensia Josefina Reyes Cervantes      }\\
\noindent El problema de suavizamiento de series de tiempo económicas ha sido abordado con la aplicación del filtro de Hodrick-Prescott. La elección de la constante de suavizamiento juega un papel indispensable en la solución de este problema.  Originalmente Hodrick y Prescott propusieron un valor específico para dicha constante y  a pesar de que este valor ha sido muy utilizado por  los analistas de la economía, también es cierto que ha sido cuestionado  por su carácter empírico. Por otra parte, Paige y Trindade demostraron en 2010 que el filtro de Hodrick y Prescott equivale a un caso especial de splines penalizados para suavizamiento, con lo cual se abre una variedad de posibilidades para la elección de la constante de suavizamiento para el problema de HP, ya que existen muchas propuestas en el ámbito de splines. En la plática se presenta el método propuesto en el 2007 por Guerrero, el cual determina la constante de suavizamiento mediante una función del porcentaje deseado de suavidad para la tendencia.  Se compara en particular con los métodos de Máxima Verosimilitud y Máxima Verosimilitud Restringida, utilizados para escoger la constante de suavizamiento mediante splines penalizados dentro del contexto de modelos lineales mixtos.
%%%%%%%%%%%%1360%%%%%%%%%%%
\subsection{\sffamily Detección de Efectos Activos y Outliers en Factoriales No-Replicados {\footnotesize (CI, Pos)}} \label{reg-1360} \index{De La Vara Salazar Roman}
\noindent {\bfseries Roman  De La Vara Salazar}, {\tt delavara@cimat.mx}  {\slshape (Centro de Investigación en Matemáticas (CIMAT))}\\
          \noindent Primero se muestra mediante ejemplos el impacto que puede tener un outlier en el desempeño de las técnicas para el análisis de factoriales no replicados que no consideran esa posibilidad. Enseguida se presentan los métodos que se han propuesto para el análisis de estos experimentos, y que consideran la detección explícita tanto de efectos activos como de outliers. Se propone un nuevo método y se estudia su desempeño con ejemplos y simulación de monte carlo.
%%%%%%%%%%%%1595%%%%%%%%%%%
\subsection{\sffamily La actitud de los estudiantes hacia la estadística. Un estudio empírico a partir de las variables de la escala EATS {\footnotesize (RI, Inv)}} \label{reg-1595} \index{Escalera Chávez Milka Elena}
\noindent {\bfseries Milka Elena Escalera Chávez}, {\tt milkaech@uaslp.mx}  {\slshape (Universidad Autónoma de San Luis Potosí (UASLP))}\\
          \noindent En el estudio, se analiza la actitud de los alumnos hacia la estadística por medio de un modelo que considera las variables  propuestas por Auzmendi (1992). Se comprobó si los constructos: utilidad, motivación, agrado, confianza y ansiedad  influyen en la actitud del alumno hacia la estadística. Se encuestó a 298 estudiantes de la Universidad Cristóbal Colón mediante el cuestionario propuesto por Auzmendi. El análisis de los datos se llevó a cabo mediante un modelo de ecuaciones estructurales con el software AMOS. Los resultados apoyan el modelo propuesto por Auzmendi de 5 componentes, sin embargo un dato relevante que deja ver este resultado es que existe un modelo alternativo (CFI= 0.907) que se ajusta mejor al modelo propuesto (CFI=0.885). Además, de los 25 indicadores planteados sólo 22 tienen un rango aceptable y dos de los indicadores -- ítem 9 e ítem 2-- deben ser considerados en el constructo de ansiedad y de utilidad respectivamente.  Palabras clave: Componentes, utilidad, motivación, agrado, confianza y ansiedad
%%%%%%%%%%%%383%%%%%%%%%%%
\subsection{\sffamily Detección de fallas en multiceldas de proceso mediante análisis multivariado {\footnotesize (RT, Pos)}} \label{reg-383} \index{Gómez Ramos Adriana Monserrat}
\noindent {\bfseries Adriana Monserrat Gómez Ramos}, {\tt amgomez@live.com.mx}  {\slshape (Instituto Tecnológico de Saltillo)}\\
          \noindent  Debido a la complejidad que presentan en la actualidad los procesos de manufactura por las múltiples variables relacionadas dentro de sus procesos, algunas técnicas conocidas de la estadística multivariada dificultan la interpretación e identificación rápida y sencilla para detectar fallas durante los mismos, debido a la naturaleza intrínseca de sus datos.  Es por ello que se propone el uso de análisis de componentes independientes (ICA) y del análisis de componentes principales (PCA), como una propuesta de metodología combinada para la detección, mejora y control de fallas durante un proceso.  La idea de usar PCA e ICA es que la información generada durante los procesos de manufactura en la mayoría de los casos tiene un comportamiento no Gaussiano, de ahí las limitaciones de las técnicas clásicas de análisis como PCA entre otras. Un caso de estudio de una empresa de la localidad es evaluado para identificar la causa raíz de las variables que producen los fallos durante el proceso.
%%%%%%%%%%%%1194%%%%%%%%%%%
\subsection{\sffamily Pronóstico de la Humedad Usando un Modelo de Análisis de Regresión {\footnotesize (CDV, 2Lic)}} \label{reg-1194} \index{Pérez Cortes Pedro}
\noindent {\bfseries Pedro  Pérez Cortes}, {\tt pecort4@hotmail.com}  {\slshape (Benemérita Universidad Autónoma de Puebla (BUAP))}\\
\noindent {\it  Coautor: Bulmaro  Juárez  Hernández        }\\
\noindent Los pronósticos del tiempo proporcionan información crítica sobre el clima futuro, existen varias técnicas que intervienen en el pronóstico del tiempo, desde la observación relativamente sencilla del cielo hasta la alta complejidad de los modelos matemáticos computarizados. Este trabajo pretende establecer un modelo estadístico paramétrico de regresión para realizar la predicción sobre un conjunto de datos los cuales corresponden a los registros meteorológicos de las variables: humedad (variable dependiente), temperatura del aire, temperatura máxima, temperatura mínima, humectación de hoja, velocidad del viento, velocidad máxima, sensación térmica, recorrido del viento, lluvia, intensidad máxima, punto rocío y evapotranspiración obtenidos en la zona de Texcoco proporcionados por la estación Meteorológica del Departamento de Irrigación de la U.A.CH.(Universidad Autónoma de Chapingo).\noindent \textbf{Palabras Clave}:\textit{\ Humedad relativa, Regresión, factores climatológicos}.
%%%%%%%%%%%%464%%%%%%%%%%%
\subsection{\sffamily Estimadores de punto de cambio en series de tiempo para procesos con cambios graduales sostenidos con parámetros desconocidos. {\footnotesize (CI, 2Lic)}} \label{reg-464} \index{López Aguilar Eduardo}
\noindent {\bfseries Eduardo  López Aguilar}, {\tt borre\_ftb@hotmail.com}  {\slshape (Facultad de Ciencias Físico Matemáticas, Universidad Autónoma de Nuevo León (UANL))}\\
\noindent {\it  Coautores: Bertha Elidia Gutiérrez Nájera, Álvaro Eduardo Cordero Franco, Víctor Gustavo Tercero Gómez    }\\
\noindent En el monitoreo de procesos es común encontrar el uso de  cartas de control,  las cuales asisten en la detección de causas asignables de variación. Sin embargo cuando causas asignables crean cambios sostenidos la estimación del momento inicial del cambio se debe realizar mediante estimadores de punto de cambio. En este trabajo se analizan series de tiempo normales con cambios graduales de tendencia lineal en media o varianza y con parámetros desconocidos. Se presentan el desarrollo de estimadores de máxima verosimilitud para el punto de cambio y los parámetros del modelo.
%%%%%%%%%%%%1602%%%%%%%%%%%
\subsection{\sffamily Análisis de Regresión para la estimación del secuestro de carbono orgánico en suelos. {\footnotesize (FALTA, FALTA)}} \label{reg-1602} \index{López Pineda Gabriela}
\noindent {\bfseries Gabriela  López Pineda}, {\tt beyota\_gab22@hotmail.com}  {\slshape (Facultad de ciencias Físico Matemáticas, Benemérita Universidad Autónoma de Puebla.(FCFM-BUAP))}\\
\noindent {\it  Coautores: Gladys Linares Fleites, Hortensia Josefina Reyes Cervantes      }\\
\noindent Como resultado del aumento de concentraciones de gases de efecto invernadero, existen evidencias científicas que sugieren que el clima global se está alterando en este siglo. El principal responsable del cambio climático global es elCO2, que tiene entre sus fuentes emisoras la deforestación y la destrucción delos suelos. El carbono orgánico del suelo (COS) es un gran y activo reservorio que se encuentran en los ecosistemas forestales ya que pueden absorber cantidades significativas de CO2, por lo que hay un gran interés por incrementar el contenido de carbono en estos ecosistemas. El objetivo de este trabajo es mostrar las posibilidades que ofrece el modelo de regresión lineal múltiple para estudiar el cambio de uso desuelo, en la Zona de la Caldera de Teziutlán en el estado de Puebla. Un problema serio que puede influir mucho sobre la utilidad del modelo de regresión es la multicolinealidad, o dependencia lineal entre las variables independientes de la regresión. Como solución al problema de multicolinealidad en regresión múltiple, se  presentan y comparan las técnicas de regresión de componentes principales (RCP) y la regresión de componentes desde el enfoque de mínimos cuadrados parciales (PLS) y finalmente, se ilustran las metodologías con una base de datos.
%%%%%%%%%%%%978%%%%%%%%%%%
\subsection{\sffamily Un avance en la comparación estocástica de unas matrices aplicadas a series de datos Meteorológicos {\footnotesize (RT, 1Lic)}} \label{reg-978} \index{Vargas Octavio Gutiérrez}
\noindent {\bfseries Octavio Gutiérrez Vargas}, {\tt octavio.mat@gmail.com}  {\slshape (Universidad de Guadalajara (U de G))}\\
          \noindent El inter\'es de la sociedad en relaci\'on a los recientes acontecimientos con respecto a nuestro entorno, son cada d\'ia mas grandes. El estudiar fenómenos meteorol\'ogicos tales como huracanes, inundaciones, sequias, por citar algunos, son d\'ia a d\'ia estudiados con mayor \'enfasis desarrollando una mayor concientizaci\'on por parte de los investigadores hacia la sociedad. Cabe destacar, tanto países como ciudades de otras partes del mundo han implementado la utilizaci\'on de programas especializados al estudio y pronostico del cambio climatico.Existen diversos modelos f\'isico-matem\'aticos para el estudio del clima, pueden ser tanto Modelos Clim\'aticos Globales (MCG) como Modelos Clim\'aticos Regionales (MCR), del an\'alisis de estos conseguimos escenarios clim\'aticos; es decir, representaciones probabil\'isticas las cuales nos indican c\'omo ser\'ian los comportamientos del clima en una cantidad determinada de a\~nos, en base a datos hist\'oricos. Ejemplo del primero encontramos ECHAM4 A2 y B2 (A2 y B2, son ``familias'' de escenarios experimentales [1960-2100], d\'onde la primera nos describe escenarios con un mundo heterog\'eneo, con una poblaci\'on en continuo crecimiento econ\'omico, el segundo, con menor crecimiento poblacional y desarrollo econ\'omico intermedio).El otro, PRECIS, (Provinding Regional Climates for Impacts Studies, \'este desarrollado por el Hadley Center de la oficina de Meteorolog\'ia de Reino Unido. Apoyándonos con el PRECIS, analizaremos la base de datos que el programa nos provee, mismos que ser\'an comparados con datos registrados en la base de datos de rean\'alisis del NOAA (National Oceanic and Atmospheric Administration), para realizar una comparaci\'on entre ambas. Las herramientas para el estudio de nuestro entorno son muy amplias y existen programas, f\'ormulas o manuales que pueden decir con cierta  precisi\'on c\'omo y cu\'ando va a suceder un meteoro. Para nuestro objetivo vamos a recurrir a un \'area de las matem\'aticas con creciente inter\'es, tal \'area es la estad\'istica, que si bien no da un resultado exacto, provee de soluci\'ones aproximadas a problemas donde no existe un m\'etodo definido para la soluci\'on de los mismos.
%%%%%%%%%%%%1462%%%%%%%%%%%
\subsection{\sffamily Entropy and purity of partially coherent beam {\footnotesize (CI, Inv)}} \label{reg-1462} \index{Silva Barranco Javier}
\noindent {\bfseries Javier  Silva Barranco}, {\tt jvr\_silva@inaoep.mx}  {\slshape (Instituto Nacional de Astrofísica Óptica y Electrónica (INAOE).  Departamento de Óptica.)}\\
\noindent {\it  Coautores: Patricia  Martínez Vara, Gabriel  Martínez Niconoff      }\\
\noindent We describe the nxn coherence matrix for partially coherent beams, the elements of each file are interpreted as random variables that represent an interaction measurement for the interaction between completely coherent beams. We associate an entropy measurement for each file. Using these values an order relation is identified that allows assigning the purity degree for partially coherent beams. Experimental results are shown.
%%%%%%%%%%%%1178%%%%%%%%%%%
\subsection{\sffamily Análisis del comportamiento de la radiación solar usando métodos de series de tiempo {\footnotesize (RT, 2Lic)}} \label{reg-1178} \index{Matías Castillo Brenda Catalina}
\noindent {\bfseries Brenda Catalina Matías Castillo}, {\tt caty\_b26@hotmail.com}  {\slshape (Benemérita Universidad Autónoma de Puebla (BUAP))}\\
\noindent {\it  Coautor: Bulmaro  Juárez Hernández        }\\
\noindent La radiación solar es la energía transmitida por el sol a través de ondas electromagnéticas, por medio de ella se pueden inferir procesos de transferencia de energía en las diferentes capas atmosféricas que se manifiestan como fenómenos climáticos que pueden cuantificarse. Además, permite comprender otros fenómenos meteorológicos, como la humedad, la temperatura, etc. En el presente trabajo se realiza un estudio de la radiación solar mediante el uso del análisis en series de tiempo, trabajándose  con modelos ARIMA usando el enfoque de Box-Jenkins para series estacionales. Se encuentra el modelo que mejor se ajusta a los datos de radiación  y se realizan pronósticos. Lo anterior se lleva a cabo sobre una base de datos de radiación solar, registrados cada media hora desde el 17-05-2003 al 28-10-2010. Se genera una serie de tiempo de los promedios semanales de los valores de la radiación solar, encontrándose un modelo que mejor ajusta a  dichos datos, para posteriormente realizar pronósticos que se comparan con los valores observados en algunos meses del año 2010. Palabras clave: Radiación solar, Box-Jenkins, estacionalidad, estacionaridad
%%%%%%%%%%%%735%%%%%%%%%%%
\subsection{\sffamily Optimización, no-unicidad y robustez en la toma de decisiones bajo incertidumbre. {\footnotesize (CI, 1Lic)}} \label{reg-735} \index{Lemus-Rodríguez Enrique}
\noindent {\bfseries Enrique  Lemus-Rodríguez}, {\tt enrique.lemus@gmail.com}  {\slshape (Escuela de Actuaría Universidad Anáhuac México Norte)}\\
          \noindent En diversos contextos un problema de inferencia plantea como un problema de optimización. Al contrario de otros contextos, en la optimización no siempre se da la unicidad del óptimo, y existen muchos casos naturales, incluso algunos íntimamente relacionados con la estadística, como es el de la mediana. El objetivo de esta plática es plantear la relación de la no-unicidad y el problema de construir métodos robustos de toma decisiones bajo incertidumbre como una de las fronteras fértiles de interacción entre ramas hermanas, precisamente la optimización y la estadística.
%%%%%%%%%%%%1509%%%%%%%%%%%
\subsection{\sffamily Modelación espacio temporal de eventos extremos usando procesos Max-Stable {\footnotesize (RT, Pos)}} \label{reg-1509} \index{Muñiz Merino Lucila}
\noindent {\bfseries Lucila  Muñiz Merino}, {\tt muniz.lucila@colpos.mx}  {\slshape (Colegio de Postgraduados(COLPOS))}\\
\noindent {\it  Coautores: Humberto  Vaquera Huerta, José Aurelio Villaseñor Alva, Elizabeth  González Estrada, Barry  Arnold  }\\
\noindent Varios fenómenos naturales extremos tales como precipitación, temperaturas, vientos, niveles de contaminación y nevadas, entre otros, representan severos riesgos para la población, la economía y el medio ambiente. Este trabajo tiene como objetivo determinar la tendencia espacio-temporal de eventos extremos. Lo anterior se realiza mediante uso de modelos ``max-stable'' a los valores máximos determinados en bloques de observaciones. Se utiliza el criterio de Takeuchi para elegir el mejor modelo paramétrico que ajusta el F-Madograma  para los datos de periodo de tiempo. La tendencia del evento extremo en el tiempo se determina por medio de regresión sobre uno de los parámetros del modelo ajustado. Lo anterior se ilustra con datos de contaminación por ozono en el valle de México durante 1998-2010.
%%%%%%%%%%%%689%%%%%%%%%%%
\subsection{\sffamily Inferencia fiducial para las distribuciones gamma y exponencial truncada {\footnotesize (CI, Inv)}} \label{reg-689} \index{Nájera Rangel Edilberto}
\noindent {\bfseries Edilberto  Nájera Rangel}, {\tt edilberto.najera@ujat.mx}  {\slshape (Universidad Juárez Autónoma de Tabasco (UJAT), División Académica de Ciencias Básicas.)}\\
\noindent {\it  Coautor: Edilberto  Nájera Rangel        }\\
\noindent En esta plática se presentarán procedimientos de inferencia tanto desde el punto de vista bayesiano como del clásico. Para ambas distribuciones, a través de varios ejemplos, numéricamente se mostrará que la distribución final y la fiducial del parámetro respectivo prácticamente coinciden.
%%%%%%%%%%%%970%%%%%%%%%%%
\subsection{\sffamily Inferencia en series de tiempo ambientales de valores extremos bajo censura {\footnotesize (CI, Pos)}} \label{reg-970} \index{Estrada-Drouaillet Benigno}
\noindent {\bfseries Benigno  Estrada-Drouaillet}, {\tt benigno@colpos.mx}  {\slshape (Colegio de Postgraduados (COLPOS))}\\
\noindent {\it  Coautores: HUMBERTO  VAQUERA HUERTA, SERGIO  PEREZ ELIZALDE      }\\
\noindent Las partículas suspendidas en zonas urbanas, constituyen un factor de riesgo para la salud. El monitoreo de los niveles extremos de este contaminante es de suma importancia ya que estos son los que de manera directa afectan en mayor medida la salud. En las estaciones de monitoreo ambiental un problema que se presenta con frecuencia es la falla de los equipos lo cual trae como consecuencia problemas de series incompletas o censuradas.   En el presente trabajo  se utiliza la distribución de valores extremos generalizada (DGEV)   para modelar los  datos de contaminación ambiental censurados. Previo a la construcción de la verosimilitud, se formaron grupos de observaciones para calcular el valor máximo por grupo y de esta manera lograr independencia entre las observaciones (Método Máximo de Bloque). Para estimar parámetros se hace una modificación a la verosimilitud  de tal manera que en esta se incluyan los datos censurados. Una vez construida la verosimilitud se procede a la estimación de los parámetros de la distribución mediante métodos numéricos (Nelder-Mead). Para ejemplificar la metodología, se emplean datos de partículas suspendidas menores a 10 micrómetros (PM10) que se obtuvieron de la estación Tlalnepantla que pertenece al Sistema de Monitoreo Atmosférico (SIMAT) de la Secretaria del Medio Ambiente del Gobierno del Distrito Federal. Los resultados nos permiten presentar las tendencias en los niveles muy altos del contaminarte en los últimos años.
%%%%%%%%%%%%427%%%%%%%%%%%
\subsection{\sffamily Uso de rangos para estimación no paramétrica de punto de cambio en series de tiempo {\footnotesize (RI, Inv)}} \label{reg-427} \index{Morales Brenda Lizeth}
\noindent {\bfseries Brenda Lizeth Morales}, {\tt lizethm\_283@hotmail.com}  {\slshape (Universidad Autónoma de Nuevo León (UANL))}\\
\noindent {\it  Coautores: Ana Elizabeth Ramos, Álvaro Eduardo Cordero, Víctor Gustavo Tercero    }\\
\noindent Una estrategia utilizada para hacer análisis de punto de cambio en series de tiempo es mediante el supuesto de que sus observaciones siguen una función de distribución conocida, donde el desconocimiento está limitado a los parámetros de dicha función. Sin embargo cuando la distribución teórica es desconocida, o las técnicas paramétricas correspondientes no han sido desarrolladas o se ignoran, se requiere cambiar a un enfoque no paramétrico. En este artículo se propone un estimador no paramétrico basado en la técnica de transformación de rangos para el momento en que series de tiempo sufren un cambio sostenido gradual o escalonado.
%%%%%%%%%%%%1387%%%%%%%%%%%
\subsection{\sffamily Confiabilidad y validez de los instrumentos de investigación para la recolección de datos {\footnotesize (RT, 2Lic)}} \label{reg-1387} \index{Solís Baas Neyfis Vanessa}
\noindent {\bfseries Neyfis Vanessa Solís Baas}, {\tt neyfis\_37@hotmail.com}  {\slshape (Benemérita Universidad Autónoma de Puebla (BUAP))}\\
          \noindent \small{Generalmente, en los estudios utilizados en las Ciencias Sociales, los datos se obtienen por medición de las variables de interés. Existen muchos requisitos que deben llenar los instrumentos de medición, ya que si no los llenan, los datos recolectados tendrán limitaciones importantes. Entre estos requisitos o cualidades están la confiabilidad y la validez del instrumento. La confiabilidad y la validez de un instrumento no son cualidades completamente independientes. Un dispositivo de medición que no sea confiable no puede ser válido, pues si es errático, incongruente e inexacto tampoco medirá con validez el atributo en cuestión. Siempre que se requiere recopilar información en la realización de los trabajos de investigación, el investigador se enfrenta a la problemática de qué tipos de instrumentos emplear o si realmente hay uno adecuado, de manera que permitan recabar información confiable y válida, de modo que proporcione un fundamento relevante para el logro de los objetivos planteados y sustente los hallazgos que se realicen con las investigaciones. El valor de un estudio depende de que la información recabada refleje lo más fidedignamente el evento investigado, dándole una base real para obtener un producto de investigación de calidad. En este avance de tesis se llevará a cabo una introducción del estudio exhaustivo de las técnicas y métodos estadísticos existentes en la literatura sobre validez y confiabilidad de instrumentos de medición. Estos métodos se  aplicarán a varios instrumentos de medición de investigaciones que se llevan a cabo en la Facultad de Enfermería y en la Dirección General de Bibliotecas de la BUAP, con el propósito de hacer comparaciones desde el punto de vista estadístico sobre las bondades de las diferentes técnicas y métodos según el tipo de instrumento de medición de que se trate.}
%%%%%%%%%%%%368%%%%%%%%%%%
\subsection{\sffamily Análisis de correspondencias múltiples para la identificación de perfiles sociodemográficos de los migrantes internos e internacionales en México, 2000 y 2010. {\footnotesize (RT, Inv)}} \label{reg-368} \index{Rodríguez Abreu Mauricio}
\noindent {\bfseries Mauricio  Rodríguez Abreu}, {\tt mrabreu22@gmail.com}  {\slshape (El Colegio de México (COLMEX))}\\
          \noindent En el estudio de las migraciones en México, por lo general, se ha privilegiado el análisis de uno u otro tipo de migración. Pocos son los trabajos que observan las características de los migrantes internos e internacionales en el país de manera simultánea. De manera histórica se ha reconocido que cada tipo de migración incorpora a poblaciones con características sociodemográficas muy particulares y que éstas son diferentes a las observadas en otras poblaciones migrantes. Sin embargo, también se reconoce que los cambios que ha experimentado el fenómeno migratorio mexicano y las tendencias, así como la creciente urbanización de la población migrante, podría estar generando cambios en los perfiles de los diferentes migrantes. Mediante el uso del análisis de correspondencias múltiples se busca identificar qué variables sociodemográficas están más relacionados con cada tipo de migración. Para tal fin, se analizan las poblaciones de migrantes internos, migrantes de retorno de Estados Unidos y emigrantes a dicho país en los años 2000 y 2010.Los resultados del análisis señalan que no sólo cada tipo de migración tiene características particulares para la población inmersa en los desplazamientos, sino que las diferencias observadas son mayores en el año 2010.
%%%%%%%%%%%%745%%%%%%%%%%%
\subsection{\sffamily Uso de Regresión Lineal para Estimar Datos Perdidos {\footnotesize (CI, 1Lic)}} \label{reg-745} \index{Sánchez Díaz Silvia}
\noindent {\bfseries Silvia  Sánchez Díaz}, {\tt silviasandi@profesores.valles.udg.mx}  {\slshape (Universidad de Guadalajara)}\\
          \noindent El análisis estadístico de datos observados en fenómenos naturales, con frecuencia se presenta la existencia de datos perdidos y que no es posible recuperarlos. En los últimos años se han desarrollado trabajos de investigación, en los cuales se aplican métodos para analizar las bases de datos con observaciones perdidas, con el fin de obtener una estimación que no afecte drásticamente las conclusiones finales del estudio, desafortunadamente muchas de las técnicas no logran el objetivo y  carecen de fundamento o introducen errores que afectan resultados y conclusiones. Desafortunadamente, cualquier análisis de estos datos se ve limitado por esos ``huecos'' que generan los datos perdidos, por lo que las metodologías propuestas en la literatura para estimarlos han tomado fuerza en años recientes, proponiéndose técnicas desde imputación por medias y reemplazo anual, hasta modelos más sofisticados como lo son los modelos de regresión, modelos geo-estadísticos, análisis espectral, métodos de interpolación y modelos multivariados, entre otros. Se   utiliza el método de regresión lineal para estimar datos perdidos a los datos  históricos disponibles de la Red Automática de Monitoreo de la Zona Metropolitana de Guadalajara, Jalisco, México (ZMG), que cuenta con 8 estaciones (Las Águilas, Atemajac, Centro, Loma Dorada, Miravalle, Oblatos, Tlaquepaque y Vallarta),  en las cuales se mide automáticamente concentraciones horarias de contaminantes atmosféricos. El análisis de regresión múltiple permite estimar los datos pedidos y determinar la mejor relación que permita explicar el comportamiento de la variable dependiente, sobre un conjunto de variables independientes. Para el acomodo de los datos se utilizará  Excel, posteriormente Matlab para obtener los estimadores de los parámetros del modelo de regresión lineal múltiple.
%%%%%%%%%%%%1077%%%%%%%%%%%
\subsection{\sffamily Modelos Ajustados de Comportamiento para Riesgo Crediticio {\footnotesize (RT, Pos)}} \label{reg-1077} \index{Sotelo Chávez Javier}
\noindent {\bfseries Javier  Sotelo Chávez}, {\tt zottelo@hotmail.com}  {\slshape (Universidad Autónoma Metropolitana Iztapalapa (UAM-I))}\\
          \noindent Una de las principales incertidumbres de las instituciones financieras es conocer cuál es el nivel de riesgo de sus clientes. Para esto se requiere de una herramienta que sea capaz de asignar una calificación precisa a los usuarios. El Credit Scoring es una de las técnicas más exitosas utilizadas para este fin la cual combina herramientas estadísticas, investigación de operaciones y minería de datos para modelar el riesgo crediticio del consumidor. Con este mecanismo se toman decisiones como: ¿Quién debería obtener un crédito? Y ¿Cómo administrar los créditos ya existentes? Los modelos de Credit Scoring que resuelven el segundo tipo de decisión se conocen como Modelos de Comportamiento (Behavioral Scoring).   Existen técnicas generales para construir Modelos de Comportamiento, sin embargo el resultado obtenido con diferentes instituciones no siempre es favorable. Esto se debe a que cada una de ellas cuenta con distintas políticas para administrar su cartera de clientes lo cual implica que se requiere crear Modelos particulares que se ajusten a las necesidades de cada empresa.   El objetivo de este trabajo es desarrollar técnicas matemáticas para construir Modelos Ajustados de Comportamiento que maximicen la utilidad y minimicen el riesgo que asume la empresa justificando su funcionalidad con una base teórica sustentable y un ejercicio práctico que muestre resultados sobresalientes en comparación con la técnica tradicional.
%%%%%%%%%%%%716%%%%%%%%%%%
\subsection{\sffamily Acerca de la construcción de índices para la medición social: el caso del índice de marginación de CONAPO {\footnotesize (RI, Inv)}} \label{reg-716} \index{Vargas Chanes Delfino}
\noindent {\bfseries Delfino  Vargas Chanes}, {\tt dvchanes@gmail.com}  {\slshape (Programa Universitario de Estudios del Desarrollo, Universidad Nacional Autónoma de México (UNAM))}\\
\noindent {\it  Coautor: Fernando  Cortés        }\\
\noindent Frecuentemente se construyen índices para medir el desarrollo del país y con base en estos índices se toman decisiones de política pública. Dichos índices son de gran utilidad pero en muchas ocasiones éstos no tienen las propiedades deseables para ser utilizados propiamente. En la plática se tratará el caso del índice de marginación elaborado por COANPO, el cual se basa en el Análisis de Componentes Principales  y se ejemplificará la manera de remplazarlo por uno distinto basado en el Análisis Factorial Confirmatorio el cual tiene las propiedades deseables (e.g, invarianza factorial y ser comparable en el tiempo). Adicionalmente, se plantean propuestas metodológicas específicas para el uso de este índice alternativo en análisis longitudinal y transversal para fines de política pública.
%%%%%%%%%%%%663%%%%%%%%%%%
\subsection{\sffamily ?`Los muertos nos dicen sus patrones demográficos? Aplicaciones de la estadística-demográfica a los estudios de las poblaciones antiguas {\footnotesize (CDV, Inv)}} \label{reg-663} \index{Ortega Muñoz Allan}
\noindent {\bfseries Allan  Ortega Muñoz}, {\tt allanortega@yahoo.com}  {\slshape (Instituto Nacional de Antropología e Historia (INAH) Centro INAH Quintana Roo)}\\
          \noindent La presente ponencia tiene el objetivo de mostrar la simulación del patrón demográfico de la parroquia, del siglo XIX, Santa María la Redonda ubicada en la ciudad de México a partir de dos metodologías, la demografía histórica y la paleodemografía, cuya finalidad es evaluar las similaridades de los resultados estadístico-demográficos de ambas metodologías. Para el primer caso se empleó los registros bautismales (1,676 individuos) y de defunción (2,067 individuos) parroquiales abarcando los años 1840-1849. Para la segunda se empleó una serie de 342 individuos esqueletizados provenientes del extinto Panteón de Santa Paula del siglo XIX, ubicado en los linderos de la misma parroquia. Los resultados obtenidos muestran diferencias en todos los análisis demográficos, por lo que se tienen dos escenarios demográficos posibles: uno con mayor fecundidad (de la demografía histórica) y por ende, mayor mortalidad infantil. Mientras que, con la paleodemografía, los valores son más elevados pero no necesariamente erróneos, pues al ser comparados con otros estudios realizados para la época y de diferentes regiones de México se encuentran éstos muy cercanos. La conclusión es que si bien no son complementarias ambas metodologías, si pueden ser adecuadas para el estudio demográfico de poblaciones antiguas y pueden presentarse como formas de analizar, en un amplio panorama, la dinámica demográfica pasada.
%%%%%%%%%%%%460%%%%%%%%%%%
\subsection{\sffamily Estimadores de punto de cambio en series de tiempo con distribuciones Bernoulli y Binomial con parámetros desconocidos {\footnotesize (CI, 2Lic)}} \label{reg-460} \index{Urizar Villanueva Viridiana}
\noindent {\bfseries Viridiana  Urizar Villanueva}, {\tt virip.11@hotmail.com}  {\slshape (Facultad de Ciencias Físico Matemáticas FCFM, Universidad Autónoma de Nuevo León (U.A.N.L.))}\\
\noindent {\it  Coautores: Karen Lizet Gómez Vaca, Víctor Gustavo Tercero Gómez, Álvaro Eduardo Cordero Franco    }\\
\noindent Los cuadros de control son herramientas estadísticas que ayudan a monitorear procesos y a analizar si dichos procesos están o no en control estadístico. Estas gráficas son muy efectivas al detectar que un proceso se salió de control, pero no lo son tanto en detectar el momento inicial en que el proceso se salió de control. Para la estimación del momento inicial de cambio se utiliza el análisis de punto de cambio, cuyos estimadores pueden usarse en conjunto con cuadros de control. En la siguiente investigación se analizan series de tiempo que siguen una distribución Bernoulli o Binomial, en la que ocurre un cambio en un momento desconocido con parámetros iniciales también desconocidos. El objetivo de esta investigación es estimar dicho momento y los parámetros antes y después del cambio mediante el método de máxima verosimilitud.
%%%%%%%%%%%%1155%%%%%%%%%%%
\subsection{\sffamily Analyzing the geographic diffusion of homicides in Mexico through spatial statistics techniques {\footnotesize (CI, Inv)}} \label{reg-1155} \index{Flores Segovia Miguel Alejandro}
\noindent {\bfseries Miguel Alejandro Flores Segovia}, {\tt miguel.flores@utsa.edu}  {\slshape (University of Texas at San Antonio)}\\
          \noindent In recent years Mexico has experienced unprecedented uprising levels of violence that has been attributed to the war among drug cartels and especially after the deployment of federal army forces to combat drug cartels organizations. This study applies exploratory spatial data analysis (ESDA) as well as spatial econometrics in order to investigate spatial diffusion patterns and the contextual factors associated with the increase in homicides. By using official deaths records and a recently released database of deaths allegedly attributed with ``criminal rivalry'' we are able to provide an analysis at municipal level for the years 2005-2010. We address the following research questions: To what extent the resultant diffusion of homicide rates is associated with the increased law enforcement due to federal army interventions? How does contextual level of structural variables such as poverty, marginalization, socioeconomic development, and income inequality, influence the incidence of ``criminal rivalry'' homicides?  We estimate spatial regressions that explicitly consider issues of spatial heteroscedasticity and endogeneity of regressors aiming to identify spatial regimes of high incidence in homicides. Preliminary results provide evidence of spatial regimes suggesting that the diffusion of homicides has occurred in a greater proportion within and among those states with federal army interventions and that neither poverty nor marginalization but income inequality and to a lesser extent socioeconomic development are positively associated with drug criminal rivalry homicides.


